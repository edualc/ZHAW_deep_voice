[wandb]
  project_name = zhaw_deep_voice
  group = vox2_LSTM512

[train]
  # dataset = /mnt/all1/voxceleb2/complete/dev/vox2_dev_mel
  dataset = /cluster/data/lehmacl1/datasets/vox2_dev_mel
  speaker_list = vox2_speakers_5994_dev
  n_speakers = 5994
  n_epochs = 4096
  loss = angular_margin
  batch_size = 256

  # How much of the training dataset is used for validation during training
  # (1 - validation_share) is used for training
  validation_share = 0.05

  # will not be used further
  pickle = DEPRECATED

[test]
  # dataset = /mnt/all1/voxceleb2/test/vox2_test_mel
  dataset = /cluster/data/lehmacl1/datasets/vox2_test_mel
  speaker_list = vox2_speakers_118_test
  # When comparing the clustering metrics (MR, DER, ACP, ...), 
  # a long and short sentence are built and compared, this defines
  # the ratio used for the short sentence
  short_split = 0.2
  short_utterances = False
  dominant_set = False

  # voxceleb1_dev=/mnt/all1/voxceleb1/dev/vox1_dev_mel
  # voxceleb1_test=/mnt/all1/voxceleb1/test/vox1_test_mel
  voxceleb1_dev=/cluster/data/lehmacl1/datasets/vox1_dev_mel
  voxceleb1_test=/cluster/data/lehmacl1/datasets/vox1_test_mel

  # will not be used further
  dev_pickle = DEPRECATED
  test_pickle = DEPRECATED

[active_learning]
  # Whether active learning is used during training
  enabled = False
  # How much of the training dataset is hidden away for active learning
  # (this is multiplicative from the leftover training data size AFTER
  # applying the validation_share in the train config block)
  active_learning_share = 0.8
  # How many epochs are trained before the active learning rounds are started
  epochs_before_al = 100
  # How many active learning rounds are performed in total
  # (num_epochs - epochs_before_al) // al_rounds = epochs per round
  al_rounds = 300
  # How many utterances will be added during the active learning process
  n_instances = 512
  # How many speakers will be sampled during an AL round
  # (ensure that: al_rounds * n_speakers_per_al_round >> n_speakers)
  n_speakers_per_al_round = 128

[pairwise_lstm]
  n_hidden1=512
  n_hidden2=512
  # The dense layers 1 and 2 can be adjusted, the last layer
  # is equal to n_speakers in the training set
  n_dense1=1024
  n_dense2=512
  
  # Up to which layer the network is used for final testing for
  # speaker clustering instead of the speaker classification that is 
  # being trained on during training
  out_layer = 2

  seg_size = 40
  spectrogram_height = 128
  vec_size = 1024

  # Network hyperparameters
  n_10_batches=1000
  adam_lr=0.0003
  adam_beta_1=0.9
  adam_beta_2=0.999
  adam_epsilon=1e-08
  adam_decay=0.0

[pairwise_kldiv]
  out_layer = 7
  n_epochs=1000
  batch_size=100
  epoch_batches=30
  adadelta_learning_rate=1.0
  adadelta_rho=0.95
  adadelta_epsilon=1e-6
  seg_size = 100
  spectrogram_height = 128

[angular_loss]
  margin_cosface = 0.0001
  margin_arcface = 0
  margin_sphereface = 1
  scale = 30

[luvo]
  out_layer = 10
  batch_size=128
  update_learning_rate=0.001
  update_momentum=0.9
  regression=False
  num_epochs=1000
  verbose=1
  seg_size = 50
  spectrogram_height = 128

[i_vector]
  feat_dir=../../common/data/training/i_vector/feat
  distrib_nb=2048
  rank_TV = 400
  tv_iteration = 10
  nbThread=10
  vector_size=400

[gmm]
  mixturecount=128
