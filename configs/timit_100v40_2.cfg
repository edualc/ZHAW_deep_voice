[wandb]
  project_name = zhaw_deep_voice
  group = timit_100v40_2
  job_type = development

[train]
  dataset = /mnt/all1/timit/timit_mel
  speaker_list = timit_speakers_100_50w_50m_not_reynolds
  n_speakers = 100
  n_epochs = 1000
  loss = pairwise_kldiv

  # How much of the training dataset is used for validation during training
  # (1 - validation_share) is used for training
  validation_share = 0.2

  # will not be used further
  pickle = DEPRECATED

[test]
  dataset = /mnt/all1/timit/timit_mel
  speaker_list = timit_speakers_40_clustering_vs_reynolds
  short_utterances = NOT_USED
  dominant_set = NOT_USED

  # will not be used further
  dev_pickle = DEPRECATED
  test_pickle = DEPRECATED

[active_learning]
  # Whether active learning is used during training
  enabled = True
  # How much of the training dataset is hidden away for active learning
  # (this is multiplicative from the leftover training data size AFTER
  # applying the validation_share in the train config block)
  active_learning_share = 0.75
  # How many epochs are trained before the active learning rounds are started
  epochs_before_al = 100
  # How many active learning rounds are performed in total
  # (num_epochs - epochs_before_al) // al_rounds = epochs per round
  al_rounds = 10
  # How many utterances will be added during the active learning process
  n_instances = 32
  # How many speakers will be sampled during an AL round
  # (ensure that: al_rounds * n_speakers_per_al_round >> n_speakers)
  n_speakers_per_al_round = 32

[pairwise_lstm]
  # Used for the fully connected dense layers after the LSTMs, with
  # size 10 * dense_factor; 5 * dense_factor; and n_speakers (train block)
  dense_factor = 100
  n_hidden1=256
  n_hidden2=256
  # The dense layers 1 and 2 can be adjusted, the last layer
  # is equal to n_speakers in the training set
  n_dense1=1000
  n_dense2=500
  lstm_kernel_regularization=True
  
  # Up to which layer the network is used for final testing for
  # speaker clustering instead of the speaker classification that is 
  # being trained on during training
  out_layer = 2

  seg_size = 40
  spectrogram_height = 128
  vec_size = 512

  # Network hyperparameters
  n_10_batches=1000
  adam_lr=0.001
  adam_beta_1=0.9
  adam_beta_2=0.999
  adam_epsilon=1e-08
  adam_decay=0.0

[pairwise_kldiv]
  out_layer = 7
  n_epochs=1000
  batch_size=100
  epoch_batches=30
  adadelta_learning_rate=1.0
  adadelta_rho=0.95
  adadelta_epsilon=1e-6
  seg_size = 100
  spectrogram_height = 128

[angular_loss]
  margin_cosface = 0.0001
  margin_arcface = 0
  margin_sphereface = 1
  scale = 30

[luvo]
  out_layer = 10
  batch_size=128
  update_learning_rate=0.001
  update_momentum=0.9
  regression=False
  num_epochs=1000
  verbose=1
  seg_size = 50
  spectrogram_height = 128

[i_vector]
  feat_dir=../../common/data/training/i_vector/feat
  distrib_nb=2048
  rank_TV = 400
  tv_iteration = 10
  nbThread=10
  vector_size=400

[gmm]
  mixturecount=128
